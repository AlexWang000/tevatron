{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tevatron \u00b6 Tevatron is a simple and efficient toolkit for training and running dense retrievers with deep language models. The toolkit has a modularized design for easy research; a set of command line tools are also provided for fast development and testing. A set of easy-to-use interfaces to Huggingfac's state-of-the-art pre-trained transformers ensures Tevatron's superior performance. Tevatron is currently under initial development stage. We will be actively adding new features and API changes may happen. Suggestions, feature requests and PRs are welcomed. Features \u00b6 Command line interface for dense retriever training/encoding and dense index search. Flexible and extendable Pytorch retriever models. Highly efficient Trainer, a subclass of Huggingface Trainer, that naively support training performance features like mixed precision and distributed data parallel. Fast and memory-efficient train/inference data access based on memory mapping with Apache Arrow through Huggingface datasets. Jax/Flax training/encoding on TPU Installation \u00b6 First install neural network and similarity search backends, namely Pytorch (or Jax) and FAISS. Check out the official installation guides for Pytorch , Jax and FAISS . Then install Tevatron with pip, pip install tevatron Or typically for development and research, clone this repo and install as editable, git https://github.com/texttron/tevatron cd tevatron pip install --editable . Note: The current code base has been tested with, torch==1.10.1 , faiss-cpu==1.7.2 , transformers==4.15.0 , datasets==1.17.0 Optionally, you can also install GradCache to support our gradient cache feature during training by: git clone https://github.com/luyug/GradCache cd GradCache pip install .","title":"Home"},{"location":"#tevatron","text":"Tevatron is a simple and efficient toolkit for training and running dense retrievers with deep language models. The toolkit has a modularized design for easy research; a set of command line tools are also provided for fast development and testing. A set of easy-to-use interfaces to Huggingfac's state-of-the-art pre-trained transformers ensures Tevatron's superior performance. Tevatron is currently under initial development stage. We will be actively adding new features and API changes may happen. Suggestions, feature requests and PRs are welcomed.","title":"Tevatron"},{"location":"#features","text":"Command line interface for dense retriever training/encoding and dense index search. Flexible and extendable Pytorch retriever models. Highly efficient Trainer, a subclass of Huggingface Trainer, that naively support training performance features like mixed precision and distributed data parallel. Fast and memory-efficient train/inference data access based on memory mapping with Apache Arrow through Huggingface datasets. Jax/Flax training/encoding on TPU","title":"Features"},{"location":"#installation","text":"First install neural network and similarity search backends, namely Pytorch (or Jax) and FAISS. Check out the official installation guides for Pytorch , Jax and FAISS . Then install Tevatron with pip, pip install tevatron Or typically for development and research, clone this repo and install as editable, git https://github.com/texttron/tevatron cd tevatron pip install --editable . Note: The current code base has been tested with, torch==1.10.1 , faiss-cpu==1.7.2 , transformers==4.15.0 , datasets==1.17.0 Optionally, you can also install GradCache to support our gradient cache feature during training by: git clone https://github.com/luyug/GradCache cd GradCache pip install .","title":"Installation"},{"location":"datasets/","text":"Datasets \u00b6 Dataset types \u00b6 There are usually two types of dataset format for dense retrieval training based on whether the relevancy of document is human judged or by answer exactly matching. 1. Relevancy Judged Dataset \u00b6 If the relevancy of a passage is annotated, (e.g. MS MARCO passage ranking), an instance in the dataset can usually be organized in following format: { \"query_id\" : \"<query id>\" , \"query\" : \"<query text>\" , \"positive_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ], \"negative_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ] } where the passages in positive_passages are the annotated relevant passages of the query and passages in negative_passages are usually non-relevant passages from top results of a retrieval system (e.g. BM25). 2.Exactly Matched Dataset \u00b6 If the relevancy of a passage is judged by answer exactly matching, (e.g. Natural Question), an instance in the dataset can usually be organized in following format: { \"query_id\" : \"<query id>\" , \"query\" : \"<query text>\" , \"answers\" : [ \"<answer>\" ], \"positive_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ], \"negative_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ] } where the passages in positive_passages has subsequence that exactly matches one of the answer string in answers . And passages in negative_passages are usually passages from top results of a retrieval system but doesn't have subsequence exactly matches any of answer in answers . Self-Contained Dataset \u00b6 Tevatron self-contained following common use datasets for dense retrieval. (via HuggingFace ). These datasets will be downloaded and tokenized automatically during training and encoding by setting --dataset_name <hgf dataset name> . dataset dataset HuggingFace name type MS MARCO Tevatron/msmarco-passage Relevancy Judged SciFact Tevatron/scifact Relevancy Judged NQ Tevatron/wikipedia-nq Exactly Match TriviaQA Tevatron/wikipedia-trivia Exactly Match WebQuestions Tevatron/wikipedia-wq Exactly Match CuratedTREC Tevatron/wikipedia-curated Exactly Match SQuAD Tevatron/wikipedia-squad Exactly Match Note: the self-contained datasets come with BM25 negative passages by default Take SciFact as an example: We can directly train with self-contained dataset by: python -m tevatron.driver.train \\ --do_train \\ --output_dir model_scifact \\ --dataset_name Tevatron/scifact \\ --model_name_or_path bert-base-uncased \\ --per_device_train_batch_size 16 \\ --learning_rate 1e-5 \\ --num_train_epochs 5 Then we can encode corresponding self-contained corpus by: python tevatron.driver.encode \\ --do_encode \\ --output_dir = temp_out \\ --model_name_or_path model_scifact \\ --per_device_eval_batch_size 64 \\ --dataset_name Tevatron/scifact-corpus \\ --p_max_len 512 \\ --encoded_save_path corpus_emb.pkl And encode corresponding self-contained topics by: python tevatron.driver.encode \\ --do_encode \\ --output_dir = temp_out \\ --model_name_or_path model_scifact \\ --per_device_eval_batch_size 64 \\ --dataset_name Tevatron/scifact/dev \\ --encode_is_qry \\ --q_max_len 64 \\ --encoded_save_path queries_emb.pkl Custom dataset \u00b6 To use custom dataset with Tevatron, there are two ways: 1. Raw data \u00b6 The first method is to prepare dataset in the same format as one of the above two dataset types. - If the dataset was prepared in the Relevancy Judged format, then we can directly use the data load process defined by Tevatron/msmarco-passage . - If the dataset was prepared in the Exactly Match format, then we can directly use the data load process defined by Tevatron/wikipedia-nq . For example, if we have prepared a dataset in Exactly Match format (same as Tevatron/wikipedia-nq ), with: - train data: train_dir/train_data.jsonl - dev data: dev_dir/dev_data.jsonl - corpus: corpus_dir/corpus_jsonl We can train by: python -m tevatron.driver.train \\ ... \\ --dataset_name Tevatron/wikipedia-nq \\ --train_dir train_dir \\ ... Then we can encode corpus by: python tevatron.driver.encode \\ ... \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encode_in_path corpus_dir/corpus_jsonl \\ ... And encode query by: python tevatron.driver.encode \\ ... \\ --dataset_name Tevatron/wikipedia-nq \\ --encode_in_path dev_dir/dev_data.jsonl \\ --encode_is_qry \\ ... Note: we use ... here to hide the arguments that irrelevant to dataset setting for a more clear comperision. Please see training and encoding document for detailed arguments. 2. Pre-tokenized data \u00b6 Tevatron also accept pre-tokenized custom dataset. By doing this, Tevatron will skip the tokenization step during training or encoding. The datasets need to be crafted in the format below: - Training: jsonl file with each line is a training instance, {'query': TEXT_TYPE, 'positives': List[TEXT_TYPE], 'negatives': List[TEXT_TYPE]} - Encoding: jsonl file with each line is a piece of text to be encoded, {text_id: \"xxx\", 'text': TEXT_TYPE} The TEXT_TYPE here can be either List[int] (pre-tokenized) or string (non-pretokenized). Here we encourage user to use pre-tokenized (i.e. TEXT_TYPE=List[int] ) as TEXT_TYPE=string is not supported for some tokenizer. To use custom data in pre-tokenized format, use --dataset_name json (or leave it as empty) during training and encoding. For example, if we have prepared a pre-tokenized dataset, with: - train data: train_dir/train_data.jsonl - dev data: dev_dir/dev_data.jsonl - corpus: corpus_dir/corpus_jsonl We can train by: python -m tevatron.driver.train \\ ... \\ --train_dir train_dir \\ ... Then we can encode corpus by: python tevatron.driver.encode \\ ... \\ --encode_in_path corpus_dir/corpus_jsonl \\ ... And encode query by: python tevatron.driver.encode \\ ... \\ --encode_in_path dev_dir/dev_data.jsonl \\ --encode_is_qry \\ ... Note: we use ... here to hide the arguments that irrelevant to dataset setting for a more clear comperision. Please see training and encoding document for detailed arguments.","title":"Datasets"},{"location":"datasets/#datasets","text":"","title":"Datasets"},{"location":"datasets/#dataset-types","text":"There are usually two types of dataset format for dense retrieval training based on whether the relevancy of document is human judged or by answer exactly matching.","title":"Dataset types"},{"location":"datasets/#1-relevancy-judged-dataset","text":"If the relevancy of a passage is annotated, (e.g. MS MARCO passage ranking), an instance in the dataset can usually be organized in following format: { \"query_id\" : \"<query id>\" , \"query\" : \"<query text>\" , \"positive_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ], \"negative_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ] } where the passages in positive_passages are the annotated relevant passages of the query and passages in negative_passages are usually non-relevant passages from top results of a retrieval system (e.g. BM25).","title":"1. Relevancy Judged Dataset"},{"location":"datasets/#2exactly-matched-dataset","text":"If the relevancy of a passage is judged by answer exactly matching, (e.g. Natural Question), an instance in the dataset can usually be organized in following format: { \"query_id\" : \"<query id>\" , \"query\" : \"<query text>\" , \"answers\" : [ \"<answer>\" ], \"positive_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ], \"negative_passages\" : [ { \"docid\" : \"<passage id>\" , \"title\" : \"<passage title>\" , \"text\" : \"<passage body>\" } ] } where the passages in positive_passages has subsequence that exactly matches one of the answer string in answers . And passages in negative_passages are usually passages from top results of a retrieval system but doesn't have subsequence exactly matches any of answer in answers .","title":"2.Exactly Matched Dataset"},{"location":"datasets/#self-contained-dataset","text":"Tevatron self-contained following common use datasets for dense retrieval. (via HuggingFace ). These datasets will be downloaded and tokenized automatically during training and encoding by setting --dataset_name <hgf dataset name> . dataset dataset HuggingFace name type MS MARCO Tevatron/msmarco-passage Relevancy Judged SciFact Tevatron/scifact Relevancy Judged NQ Tevatron/wikipedia-nq Exactly Match TriviaQA Tevatron/wikipedia-trivia Exactly Match WebQuestions Tevatron/wikipedia-wq Exactly Match CuratedTREC Tevatron/wikipedia-curated Exactly Match SQuAD Tevatron/wikipedia-squad Exactly Match Note: the self-contained datasets come with BM25 negative passages by default Take SciFact as an example: We can directly train with self-contained dataset by: python -m tevatron.driver.train \\ --do_train \\ --output_dir model_scifact \\ --dataset_name Tevatron/scifact \\ --model_name_or_path bert-base-uncased \\ --per_device_train_batch_size 16 \\ --learning_rate 1e-5 \\ --num_train_epochs 5 Then we can encode corresponding self-contained corpus by: python tevatron.driver.encode \\ --do_encode \\ --output_dir = temp_out \\ --model_name_or_path model_scifact \\ --per_device_eval_batch_size 64 \\ --dataset_name Tevatron/scifact-corpus \\ --p_max_len 512 \\ --encoded_save_path corpus_emb.pkl And encode corresponding self-contained topics by: python tevatron.driver.encode \\ --do_encode \\ --output_dir = temp_out \\ --model_name_or_path model_scifact \\ --per_device_eval_batch_size 64 \\ --dataset_name Tevatron/scifact/dev \\ --encode_is_qry \\ --q_max_len 64 \\ --encoded_save_path queries_emb.pkl","title":"Self-Contained Dataset"},{"location":"datasets/#custom-dataset","text":"To use custom dataset with Tevatron, there are two ways:","title":"Custom dataset"},{"location":"datasets/#1-raw-data","text":"The first method is to prepare dataset in the same format as one of the above two dataset types. - If the dataset was prepared in the Relevancy Judged format, then we can directly use the data load process defined by Tevatron/msmarco-passage . - If the dataset was prepared in the Exactly Match format, then we can directly use the data load process defined by Tevatron/wikipedia-nq . For example, if we have prepared a dataset in Exactly Match format (same as Tevatron/wikipedia-nq ), with: - train data: train_dir/train_data.jsonl - dev data: dev_dir/dev_data.jsonl - corpus: corpus_dir/corpus_jsonl We can train by: python -m tevatron.driver.train \\ ... \\ --dataset_name Tevatron/wikipedia-nq \\ --train_dir train_dir \\ ... Then we can encode corpus by: python tevatron.driver.encode \\ ... \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encode_in_path corpus_dir/corpus_jsonl \\ ... And encode query by: python tevatron.driver.encode \\ ... \\ --dataset_name Tevatron/wikipedia-nq \\ --encode_in_path dev_dir/dev_data.jsonl \\ --encode_is_qry \\ ... Note: we use ... here to hide the arguments that irrelevant to dataset setting for a more clear comperision. Please see training and encoding document for detailed arguments.","title":"1. Raw data"},{"location":"datasets/#2-pre-tokenized-data","text":"Tevatron also accept pre-tokenized custom dataset. By doing this, Tevatron will skip the tokenization step during training or encoding. The datasets need to be crafted in the format below: - Training: jsonl file with each line is a training instance, {'query': TEXT_TYPE, 'positives': List[TEXT_TYPE], 'negatives': List[TEXT_TYPE]} - Encoding: jsonl file with each line is a piece of text to be encoded, {text_id: \"xxx\", 'text': TEXT_TYPE} The TEXT_TYPE here can be either List[int] (pre-tokenized) or string (non-pretokenized). Here we encourage user to use pre-tokenized (i.e. TEXT_TYPE=List[int] ) as TEXT_TYPE=string is not supported for some tokenizer. To use custom data in pre-tokenized format, use --dataset_name json (or leave it as empty) during training and encoding. For example, if we have prepared a pre-tokenized dataset, with: - train data: train_dir/train_data.jsonl - dev data: dev_dir/dev_data.jsonl - corpus: corpus_dir/corpus_jsonl We can train by: python -m tevatron.driver.train \\ ... \\ --train_dir train_dir \\ ... Then we can encode corpus by: python tevatron.driver.encode \\ ... \\ --encode_in_path corpus_dir/corpus_jsonl \\ ... And encode query by: python tevatron.driver.encode \\ ... \\ --encode_in_path dev_dir/dev_data.jsonl \\ --encode_is_qry \\ ... Note: we use ... here to hide the arguments that irrelevant to dataset setting for a more clear comperision. Please see training and encoding document for detailed arguments.","title":"2. Pre-tokenized data"},{"location":"encoding/","text":"Encoding \u00b6 Corpus Encoding \u00b6 To encode, using the tevatron.driver.encode module. python -m tevatron.driver.encode \\ --output_dir = temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --p_max_len 128 \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encoded_save_path corpus_emb.pkl Sharded Encoding \u00b6 For large corpus, split the corpus into shards to parallelize can speed up the process. Following code did same thing as above but splits the corpus into 20 shards. for s in $( seq -f \"%02g\" 0 19 ) do python -m tevatron.driver.encode \\ --output_dir = temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --p_max_len 128 \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encoded_save_path corpus_emb_ ${ s } .pkl \\ --encode_num_shard 20 \\ --encode_shard_index ${ s } done Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq-corpus by --encode_in_path <file to encode> . (see here for details) Query Encoding \u00b6 python -m tevatron.driver.encode \\ --output_dir=temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --dataset_name Tevatron/wikipedia-nq/test \\ --encoded_save_path query.pkl \\ --q_max_len 32 \\ --encode_is_qry Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq-corpus by --encode_in_path <file to encode> . (see here for details)","title":"Encoding"},{"location":"encoding/#encoding","text":"","title":"Encoding"},{"location":"encoding/#corpus-encoding","text":"To encode, using the tevatron.driver.encode module. python -m tevatron.driver.encode \\ --output_dir = temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --p_max_len 128 \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encoded_save_path corpus_emb.pkl","title":"Corpus Encoding"},{"location":"encoding/#sharded-encoding","text":"For large corpus, split the corpus into shards to parallelize can speed up the process. Following code did same thing as above but splits the corpus into 20 shards. for s in $( seq -f \"%02g\" 0 19 ) do python -m tevatron.driver.encode \\ --output_dir = temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --p_max_len 128 \\ --dataset_name Tevatron/wikipedia-nq-corpus \\ --encoded_save_path corpus_emb_ ${ s } .pkl \\ --encode_num_shard 20 \\ --encode_shard_index ${ s } done Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq-corpus by --encode_in_path <file to encode> . (see here for details)","title":"Sharded Encoding"},{"location":"encoding/#query-encoding","text":"python -m tevatron.driver.encode \\ --output_dir=temp \\ --model_name_or_path model_nq \\ --fp16 \\ --per_device_eval_batch_size 156 \\ --dataset_name Tevatron/wikipedia-nq/test \\ --encoded_save_path query.pkl \\ --q_max_len 32 \\ --encode_is_qry Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq-corpus by --encode_in_path <file to encode> . (see here for details)","title":"Query Encoding"},{"location":"retrieval/","text":"Retrieval \u00b6 Basic Retrieval \u00b6 Tevatron implement the retrieval module based on FAISS. Currently, the search is conducted by brute force search through the index. Assume we already have query and corpus embeddings from encoding stage: query embedding: query_emb.pkl corpus embedding: corpus_emb1.pkl, corpus_emb2.pkl, ..., corpus_emb19.pkl Call the tevatron.faiss_retriever module to retrieve, python -m tevatron.faiss_retriever \\ --query_reps query.pkl \\ --passage_reps corpus_emb*.pkl \\ --depth 100 \\ --batch_size -1 \\ --save_text \\ --save_ranking_to rank.txt Encoded corpus or corpus shards are loaded based on glob pattern matching of argument --passage_reps . Argument --batch_size controls number of queries passed to the FAISS index each search call and -1 will pass all queries in one call. Larger batches typically run faster (due to better memory access patterns and hardware utilization.) Setting flag --save_text will save the ranking to a txt file with each line being qid pid score . Sharded Search \u00b6 As FAISS retrieval need to load corpus embeddings into memory, if the corpus embeddings are big, we can alternatively paralleize search over the shards. INTERMEDIATE_DIR = intermediate mkdir ${ INTERMEDIATE_DIR } for s in $( seq -f \"%02g\" 0 19 ) do python -m tevatron.faiss_retriever \\ --query_reps query_emb.pkl \\ --passage_reps corpus_emb ${ s } .pkl \\ --depth 100 \\ --save_ranking_to ${ INTERMEDIATE_DIR } / ${ s } done Then combine the results using the reducer module, python -m tevatron.faiss_retriever.reducer \\ --score_dir ${ INTERMEDIATE_DIR } \\ --query query.pkl \\ --save_ranking_to rank.txt Note: currently, reducer requires doc/query id being integer.","title":"Retrieval"},{"location":"retrieval/#retrieval","text":"","title":"Retrieval"},{"location":"retrieval/#basic-retrieval","text":"Tevatron implement the retrieval module based on FAISS. Currently, the search is conducted by brute force search through the index. Assume we already have query and corpus embeddings from encoding stage: query embedding: query_emb.pkl corpus embedding: corpus_emb1.pkl, corpus_emb2.pkl, ..., corpus_emb19.pkl Call the tevatron.faiss_retriever module to retrieve, python -m tevatron.faiss_retriever \\ --query_reps query.pkl \\ --passage_reps corpus_emb*.pkl \\ --depth 100 \\ --batch_size -1 \\ --save_text \\ --save_ranking_to rank.txt Encoded corpus or corpus shards are loaded based on glob pattern matching of argument --passage_reps . Argument --batch_size controls number of queries passed to the FAISS index each search call and -1 will pass all queries in one call. Larger batches typically run faster (due to better memory access patterns and hardware utilization.) Setting flag --save_text will save the ranking to a txt file with each line being qid pid score .","title":"Basic Retrieval"},{"location":"retrieval/#sharded-search","text":"As FAISS retrieval need to load corpus embeddings into memory, if the corpus embeddings are big, we can alternatively paralleize search over the shards. INTERMEDIATE_DIR = intermediate mkdir ${ INTERMEDIATE_DIR } for s in $( seq -f \"%02g\" 0 19 ) do python -m tevatron.faiss_retriever \\ --query_reps query_emb.pkl \\ --passage_reps corpus_emb ${ s } .pkl \\ --depth 100 \\ --save_ranking_to ${ INTERMEDIATE_DIR } / ${ s } done Then combine the results using the reducer module, python -m tevatron.faiss_retriever.reducer \\ --score_dir ${ INTERMEDIATE_DIR } \\ --query query.pkl \\ --save_ranking_to rank.txt Note: currently, reducer requires doc/query id being integer.","title":"Sharded Search"},{"location":"training/","text":"Training \u00b6 Basic Training \u00b6 To train a simple dense retriever, call the tevatron.driver.train module. Here we use Natural Questions as example. We train on a machine with 4xV100 GPU, if the GPU resources are limited for you, please train with gradient cache. python -m torch.distributed.launch --nproc_per_node = 4 -m tevatron.driver.train \\ --output_dir model_nq \\ --dataset_name Tevatron/wikipedia-nq \\ --model_name_or_path bert-base-uncased \\ --do_train \\ --save_steps 20000 \\ --fp16 \\ --per_device_train_batch_size 32 \\ --train_n_passages 2 \\ --learning_rate 1e-5 \\ --q_max_len 32 \\ --p_max_len 156 \\ --num_train_epochs 40 \\ --negatives_x_device Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq by --train_dir <train data dir> , (see here for details). Here we picked bert-base-uncased BERT weight from Huggingface Hub and turned on AMP with --fp16 to speed up training. Several command flags are provided in addition to configure the learned model, e.g. --add_pooler which adds an linear projection. A full list command line arguments can be found in tevatron.arguments . GradCache \u00b6 Tevatron adopts gradient cache technique to allow large batch training of dense retriever on memory limited GPU. Details is described in paper Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup . Adding following three flags to training command to enable gradient cache: - --grad_cache : enable gradient caching - --gc_q_chunk_size : sub-batch size for query - --gc_p_chunk_size : sub-batch size for passage For example, the following command can train dense retrieval model for Natural Question in 128 batch size but only with one GPU. CUDA_VISIBLE_DEVICES = 0 python -m tevatron.driver.train \\ --output_dir model_nq \\ --dataset_name Tevatron/wikipedia-nq \\ --model_name_or_path bert-base-uncased \\ --do_train \\ --save_steps 20000 \\ --fp16 \\ --per_device_train_batch_size 128 \\ --train_n_passages 2 \\ --learning_rate 1e-5 \\ --q_max_len 32 \\ --p_max_len 156 \\ --num_train_epochs 40 \\ --grad_cache \\ --gc_q_chunk_size 32 \\ --gc_p_chunk_size 16 Notice that GradCache also support multi-GPU setting.","title":"Training"},{"location":"training/#training","text":"","title":"Training"},{"location":"training/#basic-training","text":"To train a simple dense retriever, call the tevatron.driver.train module. Here we use Natural Questions as example. We train on a machine with 4xV100 GPU, if the GPU resources are limited for you, please train with gradient cache. python -m torch.distributed.launch --nproc_per_node = 4 -m tevatron.driver.train \\ --output_dir model_nq \\ --dataset_name Tevatron/wikipedia-nq \\ --model_name_or_path bert-base-uncased \\ --do_train \\ --save_steps 20000 \\ --fp16 \\ --per_device_train_batch_size 32 \\ --train_n_passages 2 \\ --learning_rate 1e-5 \\ --q_max_len 32 \\ --p_max_len 156 \\ --num_train_epochs 40 \\ --negatives_x_device Here we are using our self-contained datasets to train. To use custom dataset, replace --dataset_name Tevatron/wikipedia-nq by --train_dir <train data dir> , (see here for details). Here we picked bert-base-uncased BERT weight from Huggingface Hub and turned on AMP with --fp16 to speed up training. Several command flags are provided in addition to configure the learned model, e.g. --add_pooler which adds an linear projection. A full list command line arguments can be found in tevatron.arguments .","title":"Basic Training"},{"location":"training/#gradcache","text":"Tevatron adopts gradient cache technique to allow large batch training of dense retriever on memory limited GPU. Details is described in paper Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup . Adding following three flags to training command to enable gradient cache: - --grad_cache : enable gradient caching - --gc_q_chunk_size : sub-batch size for query - --gc_p_chunk_size : sub-batch size for passage For example, the following command can train dense retrieval model for Natural Question in 128 batch size but only with one GPU. CUDA_VISIBLE_DEVICES = 0 python -m tevatron.driver.train \\ --output_dir model_nq \\ --dataset_name Tevatron/wikipedia-nq \\ --model_name_or_path bert-base-uncased \\ --do_train \\ --save_steps 20000 \\ --fp16 \\ --per_device_train_batch_size 128 \\ --train_n_passages 2 \\ --learning_rate 1e-5 \\ --q_max_len 32 \\ --p_max_len 156 \\ --num_train_epochs 40 \\ --grad_cache \\ --gc_q_chunk_size 32 \\ --gc_p_chunk_size 16 Notice that GradCache also support multi-GPU setting.","title":"GradCache"}]}